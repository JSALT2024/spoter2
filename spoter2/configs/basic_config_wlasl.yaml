# training
batch_size: 8
num_workers: 2
epochs: 50
learning_rate: 0.0001
seed: 0
optimizer: adamw
scheduler: cos
# model
model_name: small
max_sequence_length: 256
positional_encoding: learnable_normal-0.08
# data
dataset_name: wlasl
data_dim: 110